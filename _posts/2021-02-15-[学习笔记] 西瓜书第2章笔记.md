---
layout: post
mathjax: true
title: "[学习笔记] 西瓜书第2章笔记"
author: "Chongxiao Li"
tags: ["machine learning", "西瓜书", "note"]
---

## 第2章 模型评估与选择

### 2.1 经验误差与过拟合
错误率：*m*个样本中有*a*个分类错误，则错误率$E=a/m$。  

精度：1-错误率，$1-a/m$。  

误差：学习器的实际预测输出与样本的真实输出间的差异。训练集上的误差称**训练误差/经验误差**，新样本上的误差称**泛化误差**。   

**过拟合**：学习器将训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降。过拟合是机器学习面临的关键障碍，只能缓解不能彻底避免。  

### 2.2 评估方法
机器学习中的**模型选择**问题的理想解决方案是对候选模型的泛化误差进行评估，选择泛化误差最小的模型。但是泛化误差不可直接获得，训练误差存在过拟合现象不适合作为标准。为此，我们需要通过实验对泛化误差进行评估，为此需使用**测试集**测试模型对新样本的判别能力，以**测试误差**近似**泛化误差**。测试集应尽可能与训练集互斥。通常我们只有一个数据集*D*，于是应对数据集进行适当处理，得到训练集*S*和测试集*T*。  

#### 2.2.1 留出法
**留出法**将数据集*D*划分为两个互斥的集合，一个作为训练集*S*，一个作为测试集*T*。  

训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。例如在分类任务中采用**分层采样**，保持样本的类别比例相似。即使如此，不同划分方法得到的模型评估结果会有差异，单次使用留出法得到的估计结果往往不够稳定可靠，一般要使用多次随即划分、重复进行实验评估后取均值作为留出法的评估结果。此外，若*S*较大，*T*较小，则训练出的模型更接近用*D*训练出的模型，但评估结果可能不够稳定准确；若*S*较小，*T*较大，则被评估的模型与用*D*训练出的模型相比差别较大，降低了评估结果大的保真性。通常采用2/3~4/5的样本用于训练，剩余用于测试。

#### 2.2.2 交叉验证法
**$k$折交叉验证法**先将数据集*D*划分为$k$个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从*D*中通过分层采样得到。每次用*k-1*个子集的并集作为训练集，剩下的子集作为测试集，从而获得$k$组测试/训练集，进行$k$次训练和测试，最后得到这$k$个测试结果的均值。$k$最常取10，此外也常取5、20等。与留出法类似，为减小因样本划分不同引入的差别，$k$折交叉验证通常随机使用不同的划分重复*p*次，最终的评估结果是*p*次$k$折交叉验证结果的均值，最常见的是10次10折交叉验证。  

若每个子集仅包含一个样本，则得到了交叉验证法的一个特例：**留一法(Leave-One-Out, LOO)**。留一法不受随机样本划分方式的影响，使用的训练集*S*与数据集*D*相比仅少一个样本，因此留一法中被实际评估的模型与期望评估的模型很相似，故而留一法的评估结果往往被认为较准确。但数据集较大时，其计算开销较大，其估计结果也未必永远比其他评估方法准确。  

#### 2.2.3 自助法
自助法以自助采样为基础，对于包含*m*个样本的数据集*D*，每次从*D*中有放回地随机抽取一个样本，将其拷贝放入*D'*中，重复*m*次后即可获得包含*m*个样本的数据集*D'*，这就是自助采样的结果。显然*D*中可能有一部分样本从未被抽取过，而*D'*中则包含重复样本。样本在m次采样中始终不被抽到的概率是$(1-\frac 1m)^m$，取极限得  
\begin{equation}\lim\limits_{m\rightarrow\infty}(1-\frac 1m)^m=\frac 1e\approx0.368\end{equation}

即约有36.8%的样本仅存在于*D*中。若将*D'*作为训练集，*D/D'*作为测试集，则实际评估的模型与期望评估的模型都采用*m*个样本进行训练，而仍有约1/3的未在训练集中出现过的样本用于测试。这样的测试结果又称为**包外估计**。  

自助法在数据集较小，难以有效划分训练/测试集时很有用，能从初始数据集中产生多个不同的训练集，对集成学习等方法有很大好处。然而，自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。   

#### 2.2.4 调参与最终模型
在进行模型评估与选择时，不仅要选择适用的学习算法，还要设定算法参数，即**参数调节**或**调参**。  

学习算法的很多参数在实数范围内取值，现实中常对每个参数选定一个范围和变化步长，在有限个参数中产生选定值。这样选定的参数值往往不是最佳值，但是这是在计算开销和性能估计之间进行折中的结果。  

在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集*D*重新训练模型，这个模型在训练过程中使用了全部样本，才是最终提交给用户的模型。  

另外，模型评估与选择中用于评估测试的数据集常称为**验证集**。在研究对比不同算法的泛化性能时，我们用**测试集**上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为**训练集**和**验证集**，基于验证集上的性能来进行模型选择和调参。  

### 2.3 性能度量
性能度量：衡量模型泛化能力的评价标准。  
对**回归任务**：均方误差，

\begin{equation}
E(f;D)=\frac{1}{m} \sum_{i=1}^{m}(f(\boldsymbol{x}_ {i})-y_{i})^{2}
\end{equation}

一般地，对数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$，

\begin{equation}
E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}}(f(\boldsymbol{x})-y)^{2} p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
\end{equation}

对**分类任务**，有以下性能度量。

#### 2.3.1 错误率与精度
**错误率**：分类错误的样本数占样本总数的比例。对样例集*D*，  

\begin{equation}
E(f ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}(f(\boldsymbol{x}_ {i}) \neq y_{i})
\end{equation}

一般地，对数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$，  

\begin{equation}
E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
\end{equation}

**精度**：分类正确的样本数占样本总数的比例。对样例集*D*，  

\begin{equation}
{acc}(f ; D) =\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}(f(\boldsymbol{x}_ {i})=y_{i}) =1-E(f ; D)
\end{equation}

一般地，对数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$，  

\begin{equation}
{acc}(f ; \mathcal{D}) =\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x})=y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} =1-E(f ; \mathcal{D})
\end{equation}

#### 2.3.2 查准率、查全率与$F1$
**查准率**：模型得到的分类结果中某一类里确实是该类的比例。“挑出的好瓜中有多少比例确实是好瓜。”   

**查全率**：模型得到的分类结果中确实是该类的占样本总体中是该类的比例。“所有好瓜中有多少比例被挑出来了。”    

对于二分类问题，  
<img src="/images/fig2-1.png" width="50%">

查准率*P*定义为  
\begin{equation}
P=\frac{T P}{T P+F P}
\end{equation}

查全率*R*定义为  
\begin{equation}
R=\frac{T P}{T P+F N}
\end{equation}

查准率与查全率是一对矛盾的度量。一般来说查准率高时，查全率往往偏低，而查全率高时，查准率往往偏低。  

根据学习器预测结果对样例进行排序，按照从最可能是正例到最不可能是正例的顺序逐个把样本作为正例进行预测，并计算出当前的查准率和查全率，即可得到**查准率-查全率曲线**，即**P-R曲线**。  
<img src="/images/fig2-2.png" width="70%">  

若一个学习器的P-R曲线将另一个学习器的P-R曲线完全包住，可以断言前者优于后者，如上图中A优于C，否则难以断言。此时一般可以比较曲线下侧的面积，但较难计算。人们设计了一些综合考虑查全率、查准率的性能度量。  

一种度量方法是平衡点(Break-Event Point, BEP)，是查准率与查全率相等时的取值。但该方法过于简化。  

更常用的度量方法是**$F1$度量**（查准率与查全率的调和平均$\frac{1}{F 1}=\frac{1}{2} \cdot(\frac{1}{P}+\frac{1}{R})$）
\begin{equation}
F 1=\frac{2 \times P \times R}{P+R}=\frac{2 \times T P}{\text { 样例总数 }+T P-T N}
\end{equation}

$F1$度量的一般形式**$F_\beta$**（查准率与查全率的加权调和平均$\frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \cdot(\frac{1}{P}+\frac{\beta^{2}}{R})$）能让我们表达出对查全率/查准率的偏好
\begin{equation}
F_{\beta}=\frac{(1+\beta^{2}) \times P \times R}{(\beta^{2} \times P)+R}
\end{equation}

其中$\beta=1$时为标准的$F1$，$\beta>1$时查全率更重要，$\beta<1$时查准率更重要。  

当我们希望在n个二分类混淆矩阵上综合考察查准率和查全率时，有两种方法。一种是**宏查准率**、**宏查全率**、**宏$F1$**，先对n个混淆矩阵计算查准率和查全率，再计算它们的均值
\begin{equation}
\operatorname{macro}-P=\frac{1}{n} \sum_{i=1}^{n} P_{i}
\end{equation}
\begin{equation}
\operatorname{macro}-R=\frac{1}{n} \sum_{i=1}^{n} R_{i}
\end{equation}
\begin{equation}
\operatorname{macro}-F 1=\frac{2 \times \operatorname{macro}-P \times \operatorname{macro}-R}{\operatorname{macro}-P+\operatorname{macro}-R}
\end{equation}

一种是**微查准率**、**微查全率**、**微$F1$**，先计算n个混淆矩阵的元素的均值，再计算查准率和查全率及$F1$
\begin{equation}
\operatorname{micro}-P=\frac{\overline{T P}}{\overline{T P}+\overline{F P}}
\end{equation}  
\begin{equation}
\operatorname{micro}-R=\frac{\overline{T P}}{\overline{T P}+\overline{F N}}
\end{equation}  
\begin{equation}
\operatorname{micro}-F 1=\frac{2 \times \operatorname{micro}-P \times \operatorname{micro}-R}{\operatorname{micro}-P+\operatorname{micro}-R}
\end{equation}  

#### 2.3.3 ROC与AUC
分类问题常通过微测试样本产生一个实值或概率预测，再将该实值与**分类阈值**进行比较进行分类。可以将这个过程看作依照该实值对测试样本进行排序，在**截断点**之前的判作正例，之后的判作反例。该截断点会依据任务需求对查全率和查准率的重视程度移动。

**受试者工作特征(Receiver Operating Characteristic, ROC)曲线**与P-R曲线类似，考察排序本身的质量好坏，进而反映学习器在不同任务下的期望泛化能力的好坏。ROC曲线的纵轴是**真正例率(True Positive Rate, TPR)**，横轴是**假正例率(False Positive Rate, FPR)**
\begin{equation}
\mathrm{TPR} =\frac{T P}{T P+F N}
\end{equation}
\begin{equation}
\mathrm{FPR} =\frac{F P}{T N+F P}
\end{equation}

可以看到，真正例率是所有正例中被判为正例的比例，假正例率是所有反例中被判为正例的比例。

ROC图中对角线对应随机猜测模型，点(0,1)对应将所有正例排在所有反例之前的理想模型。实际上测试样例是有限的，因此只能绘制出图右侧的近似ROC曲线。
<img src="/images/fig2-3.png">  

在比较两个学习器优劣时，若一个学习器的ROC曲线完全包住另一个，则可以断言前者优于后者。若ROC曲线发生交叉，可以考察ROC曲线下侧围住的面积，即**AUC(Area Under ROC Curve)**。若ROC曲线由坐标为$\{(x_{1}, y_{1}),(x_{2}, y_{2}), \ldots,(x_{m}, y_{m})\}$的点顺序连接而成，则AUC可估算为
\begin{equation}
\mathrm{AUC}=\frac{1}{2} \sum_{i=1}^{m-1}(x_{i+1}-x_{i}) \cdot(y_{i}+y_{i+1})
\end{equation}

对于给定$m^{+}$个正例和$m^{-}$个反例，令$D^{+}$和$D^{-}$表示正例集合和反例集合，考虑每一对正例和反例，若正例的预测值小于反例，记罚分1分，若相等，记罚分0.5分，计算可得排序的**损失**
\begin{equation}
\ell_{\text {rank }}=\frac{1}{m^{+} m^{-}} \sum_{\boldsymbol{x}^{+} \in D^{+}} \sum_{\boldsymbol{x}^{-} \in D^{-}}(\mathbb{I}(f(\boldsymbol{x}^{+})<f(\boldsymbol{x}^{-}))+\frac{1}{2} \mathbb{I}(f(\boldsymbol{x}^{+})=f(\boldsymbol{x}^{-})))
\end{equation}

AUC与损失之间有关系
\begin{equation}
\mathrm{AUC}=1-\ell_{\text {rank}}
\end{equation}

#### 2.3.4代价敏感错误率与代价曲线
现实任务中可能遇到不同类型的错误所造成的后果不同的情况，为了权衡不同类型错误造成的不同损失，可以为错误赋予**非均等代价**。以二分类任务为例，我们可以设置**代价矩阵**：
<img src="/images/fig2-4.png">   

前述性能度量大都隐式地假设了错误的代价均等。当我们引入非均等代价，需要采用代价敏感的性能度量。  

对错误率，可以改造为**代价敏感错误率**  
\begin{equation}
E(f;D;cost)= \frac{1}{m}(\sum_{\boldsymbol{x}_ {i} \in D^{+}} \mathbb{I}(f(\boldsymbol{x}_ {i}) \neq y_ {i}) \times \operatorname{cost}_ {01} +\sum_{\boldsymbol{x}_ {i} \in D^{-}} \mathbb{I}(f(\boldsymbol{x}_ {i}) \neq y_{i}) \times \operatorname{cost}_{10})
\end{equation}

对ROC曲线，其不能反映学习器的**期望总体代价**。我们可以采用**代价曲线**，其横轴为取值$[0,1]$的**正例概率代价**  
\begin{equation}
P(+) \text { cost }=\frac{p \times \operatorname{cost}_ {01}}{p \times \operatorname{cost}_ {01}+(1-p) \times \operatorname{cost}_{10}}
\end{equation}

其中$p$是样例为正例的概率。其纵轴为取值$[0,1]$的**归一化代价**  
\begin{equation}
\text { cost }_ {\text {norm }}=\frac{\mathrm{FNR} \times p \times \operatorname{cost}_ {01}+\mathrm{FPR} \times(1-p) \times \operatorname{cost}_ {10}}{p \times \operatorname{cost}_ {01}+(1-p) \times \operatorname{cost}_{10}}
\end{equation}

其中FPR为假正例率，FNR=1-TPR为假反例率。代价曲线的绘制方法为，以ROC曲线上每一点的坐标(TPR,FPR)计算对应的FNR，在代价平面上绘制一条从(0,FPR)到(1,FNR)的线段，该线段下侧面积表示该条件下的期望总体代价。取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价。
<img src="/images/fig2-5.png" width ="70%">  

### 2.4 比较检验
为什么要采用**统计假设检验**方法来对学习器性能进行比较？
- 希望得到泛化性能，实验只能得到在测试集上的性能，两者可能不同
- 测试集上的性能与测试集本身的选择关系很大，不同测试集结果可能不同
- 很多机器学习算法本身具有一定随机性，即使在同一个测试集上进行测试结果也可能不同  

基于统计假设检验，我们可以推断出，若在测试集上观察到学习器A比B好，则A的泛化性能是否在统计学上优于B，以及这个结论的把握有多大。
#### 2.4.1 假设检验
泛化错误率与测试错误率未必相同，但二者接近的可能性比较大，相差很大的可能性非常小，因此可以根据测试错误率估推泛化错误率。  

在包含$m$个样本的测试集上，泛化错误率为$\epsilon$的学习器被测得测试错误率$\hat{\epsilon}$的概率
\begin{equation}
P(\hat{\epsilon} ; \epsilon)=\binom{m}{\hat{\epsilon} \times m} \epsilon^{\hat{\epsilon} \times m}(1-\epsilon)^{m-\hat{\epsilon} \times m}
\end{equation}

符合**二项分布**，$P(\hat{\epsilon} ; \epsilon)$在$\epsilon = \hat{\epsilon}$时取最大，$|\epsilon - \hat{\epsilon}|$增大时减小。  
<img src="/images/fig2-6.png" width="50%">

我们可以使用**二项检验**对假设**$\epsilon \le \epsilon_0 $**进行检验，在$1-\alpha$的概率内所能观测到的最大错误率
\begin{equation}
\bar{\epsilon}=\max \epsilon \quad \text { s.t. } \quad \sum_{i=\epsilon_{0} \times m+1}^{m} \binom{m}{i} \epsilon^{i}(1-\epsilon)^{m-i}<\alpha
\end{equation}

此时若测试错误率$\hat{\epsilon}$小于临界值$\bar{\epsilon}$，则根据二项检验可得出结论：在$\alpha$的显著度下，假设$\epsilon \le \epsilon_0 $不能被拒绝，即以$1-\alpha$的置信度认为学习器的泛化错误率不大于$\hat{\epsilon}$。否则该假设可以被拒绝，即在$\alpha$的显著度下认为学习器的泛化错误率大于$\hat{\epsilon}$。  


#### 2.4.2 交叉验证*t*检验
#### 2.4.3 McNemar检验
#### 2.4.4 Friedman检验与Nemenyi检验

### 2.5 偏差与方差
