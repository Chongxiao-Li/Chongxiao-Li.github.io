---
layout: post
title: "[学习笔记] 西瓜书第2章笔记"
author: "Chongxiao Li"
tags: ["machine learning", "西瓜书", "note"]
---

## 第2章 模型评估与选择

### 2.1 经验误差与过拟合
错误率：*m*个样本中有*a*个分类错误，则错误率*E=a/m*。
精度：1-错误率，*1-a/m*。
误差：学习器的实际预测输出与样本的真实输出间的差异。训练集上的误差称**训练误差/经验误差**，新样本上的误差称**泛化误差**。
**过拟合**：学习器将训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降。过拟合是机器学习面临的关键障碍，只能缓解不能彻底避免.

### 2.2 评估方法
机器学习中的**模型选择**问题的理想解决方案是对候选模型的泛化误差进行评估，选择泛化误差最小的模型。但是泛化误差不可直接获得，训练误差存在过拟合现象不适合作为标准。为此，我们需要通过实验对泛化误差进行评估，为此需使用**测试集**测试模型对新样本的判别能力，以**测试误差**近似**泛化误差**。测试集应尽可能与训练集互斥。通常我们只有一个数据集*D*，于是应对数据集进行适当处理，得到训练集*S*和测试集*T*。
#### 2.2.1 留出法
**留出法**将数据集*D*划分为两个互斥的集合，一个作为训练集*S*，一个作为测试集*T*。
训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。例如在分类任务中采用**分层采样**，保持样本的类别比例相似。即使如此，不同划分方法得到的模型评估结果会有差异，单次使用留出法得到的估计结果往往不够稳定可靠，一般要使用多次随即划分、重复进行实验评估后取均值作为留出法的评估结果。此外，若*S*较大，*T*较小，则训练出的模型更接近用D训练出的模型，但评估结果可能不够稳定准确；若*S*较小，*T*较大，则被评估的模型与用D训练出的模型相比差别较大，降低了评估结果大的保真性。通常采用2/3~4/5的样本用于训练，剩余用于测试。
#### 2.2.2 交叉验证法
***k*折交叉验证法**先将数据集*D*划分为*k*个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从*D*中通过分层采样得到。每次用*k-1*个子集的并集作为训练集，剩下的子集作为测试集，从而获得*k*组测试/训练集，进行*k*次训练和测试，最后得到这*k*个测试结果的均值。*k*最常取10，此外也常取5、20等。与留出法类似，为减小因样本划分不同引入的差别，*k*折交叉验证通常随机使用不同的划分重复*p*次，最终的评估结果是*p*次*k*折交叉验证结果的均值，最常见的是10次10折交叉验证。
若每个子集仅包含一个样本，则得到了交叉验证法的一个特例：**留一法(Leave-One-Out, LOO)**。留一法不受随机样本划分方式的影响，使用的训练集*S*与数据集*D*相比仅少一个样本，因此留一法中被实际评估的模型与期望评估的模型很相似，故而留一法的评估结果往往被认为较准确。但数据集较大时，其计算开销较大，其估计结果也未必永远比其他评估方法准确。

#### 2.2.3 自助法
#### 2.2.4 调参与最终模型

### 2.3 性能度量
#### 2.3.1 错误率与精度
#### 2.3.2 查准率、查全率与*F*1
#### 2.3.3 ROC与AUC
#### 代价敏感错误率与代价曲线

### 2.4 比较检验
#### 2.4.1 假设检验
#### 2.4.2 交叉验证*t*检验
#### 2.4.3 McNemar检验
#### 2.4.4 Friedman检验与Nemenyi检验

### 2.5 偏差与方差
