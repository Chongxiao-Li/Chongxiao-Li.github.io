---
layout: post
mathjax: true
title: "[学习笔记] 西瓜书第2章笔记"
author: "Chongxiao Li"
tags: ["machine learning", "西瓜书", "note"]
---

## 第2章 模型评估与选择

### 2.1 经验误差与过拟合
错误率：*m*个样本中有*a*个分类错误，则错误率$E=a/m$。  

精度：1-错误率，$1-a/m$。  

误差：学习器的实际预测输出与样本的真实输出间的差异。训练集上的误差称**训练误差/经验误差**，新样本上的误差称**泛化误差**。   

**过拟合**：学习器将训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质，导致泛化性能下降。过拟合是机器学习面临的关键障碍，只能缓解不能彻底避免。  

### 2.2 评估方法
机器学习中的**模型选择**问题的理想解决方案是对候选模型的泛化误差进行评估，选择泛化误差最小的模型。但是泛化误差不可直接获得，训练误差存在过拟合现象不适合作为标准。为此，我们需要通过实验对泛化误差进行评估，为此需使用**测试集**测试模型对新样本的判别能力，以**测试误差**近似**泛化误差**。测试集应尽可能与训练集互斥。通常我们只有一个数据集*D*，于是应对数据集进行适当处理，得到训练集*S*和测试集*T*。  

#### 2.2.1 留出法
留出法将数据集*D*划分为两个互斥的集合，一个作为训练集*S*，一个作为测试集*T*。  

训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响。例如在分类任务中采用**分层采样**，保持样本的类别比例相似。即使如此，不同划分方法得到的模型评估结果会有差异，单次使用留出法得到的估计结果往往不够稳定可靠，一般要使用多次随即划分、重复进行实验评估后取均值作为留出法的评估结果。此外，若*S*较大，*T*较小，则训练出的模型更接近用*D*训练出的模型，但评估结果可能不够稳定准确；若*S*较小，*T*较大，则被评估的模型与用*D*训练出的模型相比差别较大，降低了评估结果大的保真性。通常采用2/3~4/5的样本用于训练，剩余用于测试。

#### 2.2.2 交叉验证法
*k*折交叉验证法先将数据集*D*划分为*k*个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，即从*D*中通过分层采样得到。每次用*k-1*个子集的并集作为训练集，剩下的子集作为测试集，从而获得*k*组测试/训练集，进行*k*次训练和测试，最后得到这*k*个测试结果的均值。*k*最常取10，此外也常取5、20等。与留出法类似，为减小因样本划分不同引入的差别，*k*折交叉验证通常随机使用不同的划分重复*p*次，最终的评估结果是*p*次*k*折交叉验证结果的均值，最常见的是10次10折交叉验证。  

若每个子集仅包含一个样本，则得到了交叉验证法的一个特例：**留一法(Leave-One-Out, LOO)**。留一法不受随机样本划分方式的影响，使用的训练集*S*与数据集*D*相比仅少一个样本，因此留一法中被实际评估的模型与期望评估的模型很相似，故而留一法的评估结果往往被认为较准确。但数据集较大时，其计算开销较大，其估计结果也未必永远比其他评估方法准确。  

#### 2.2.3 自助法
自助法以自助采样为基础，对于包含*m*个样本的数据集*D*，每次从*D*中有放回地随机抽取一个样本，将其拷贝放入*D'*中，重复*m*次后即可获得包含*m*个样本的数据集*D'*，这就是自助采样的结果。显然*D*中可能有一部分样本从未被抽取过，而*D'*中则包含重复样本。样本在m次采样中始终不被抽到的概率是$(1-\frac 1m)^m$，取极限得$\lim\limits_{m\rightarrow\infty}(1-\frac 1m)^m=\frac 1e\approx0.368$，即约有36.8%的样本仅存在于*D*中。若将*D'*作为训练集，*D/D'*作为测试集，则实际评估的模型与期望评估的模型都采用*m*个样本进行训练，而仍有约1/3的未在训练集中出现过的样本用于测试。这样的测试结果又称为**包外估计**。  

自助法在数据集较小，难以有效划分训练/测试集时很有用，能从初始数据集中产生多个不同的训练集，对集成学习等方法有很大好处。然而，自助法产生的数据集改变了初始数据集的分布，会引入估计偏差。因此，在初始数据量足够时，留出法和交叉验证法更常用一些。   

#### 2.2.4 调参与最终模型
在进行模型评估与选择时，不仅要选择适用的学习算法，还要设定算法参数，即**参数调节**或**调参**。  

学习算法的很多参数在实数范围内取值，现实中常对每个参数选定一个范围和变化步长，在有限个参数中产生选定值。这样选定的参数值往往不是最佳值，但是这是在计算开销和性能估计之间进行折中的结果。  

在模型选择完成后，学习算法和参数配置已选定，此时应该用数据集*D*重新训练模型，这个模型在训练过程中使用了全部样本，才是最终提交给用户的模型。  

另外，模型评估与选择中用于评估测试的数据集常称为**验证集**。在研究对比不同算法的泛化性能时，我们用**测试集**上的判别效果来估计模型在实际使用时的泛化能力，而把训练数据另外划分为**训练集**和**验证集**，基于验证集上的性能来进行模型选择和调参。  

### 2.3 性能度量
性能度量：衡量模型泛化能力的评价标准。
对**回归任务**：均方误差，  
\begin{equation}
E(f ; D)=\frac{1}{m} \sum_{i=1}^{m}\left(f\left(\boldsymbol{x}_{i}\right)-y_{i}\right)^{2}
\end{equation}
一般地，对数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$，
\begin{equation}
E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}}(f(\boldsymbol{x})-y)^{2} p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
\end{equation}

对**分类任务**，有以下性能度量。

#### 2.3.1 错误率与精度
**错误率**：分类错误的样本数占样本总数的比例。对样例集*D*，  

\begin{equation}
E(f ; D)=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right) \neq y_{i}\right)
\end{equation}  

一般地，对数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$，  

\begin{equation}
E(f ; \mathcal{D})=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x}) \neq y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x}
\end{equation}  

**精度**：分类正确的样本数占样本总数的比例。对样例集*D*，  

\begin{equation}
\begin{aligned}
\operatorname{acc}(f ; D) &=\frac{1}{m} \sum_{i=1}^{m} \mathbb{I}\left(f\left(\boldsymbol{x}_{i}\right)=y_{i}\right) \\
&=1-E(f ; D)
\end{aligned}
\end{equation}

一般地，对数据分布$\mathcal{D}$和概率密度函数$p(\cdot)$，  

\begin{equation}
\begin{aligned}
\operatorname{acc}(f ; \mathcal{D}) &=\int_{\boldsymbol{x} \sim \mathcal{D}} \mathbb{I}(f(\boldsymbol{x})=y) p(\boldsymbol{x}) \mathrm{d} \boldsymbol{x} \\
&=1-E(f ; \mathcal{D})
\end{aligned}
\end{equation}

#### 2.3.2 查准率、查全率与*F*1
#### 2.3.3 ROC与AUC
#### 代价敏感错误率与代价曲线

### 2.4 比较检验
#### 2.4.1 假设检验
#### 2.4.2 交叉验证*t*检验
#### 2.4.3 McNemar检验
#### 2.4.4 Friedman检验与Nemenyi检验

### 2.5 偏差与方差
